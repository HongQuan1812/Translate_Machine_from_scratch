{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2936819,"sourceType":"datasetVersion","datasetId":1800581},{"sourceId":8370667,"sourceType":"datasetVersion","datasetId":113569}],"dockerImageVersionId":30715,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import PyTorch\nimport torch\nfrom torch import nn\n\n# Import torchvision \nimport torchtext\n\n# Import matplotlib for visualization\nimport matplotlib.pyplot as plt\n\nimport random\n\n# Check versions\n# Note: your PyTorch version shouldn't be lower than 1.10.0 and torchtext version shouldn't be lower than 0.11\nprint(f\"PyTorch version: {torch.__version__}\\ntorchtext version: {torchtext.__version__}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"mps\" if torch.backends.mps.is_available() \\\n    else \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nraw_df = pd.read_csv(\"/kaggle/input/machine-translation-dataset-de-en/translation_train.csv\")\nraw_df.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_df['english'][8550], raw_df['german'][8550]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenization","metadata":{}},{"cell_type":"code","source":"!python -m spacy download de_core_news_sm\n!python -m spacy download en_core_web_sm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchtext.data.utils import get_tokenizer\n\nen_tokenizer = get_tokenizer(tokenizer = 'spacy', language = \"en_core_web_sm\")\nde_tokenizer = get_tokenizer(tokenizer = 'spacy', language = \"de_core_news_sm\")\n\n# apply tokenizer to our dataset\ntokenized_en_df = raw_df['english'].map(en_tokenizer)\ntokenized_de_df = raw_df['german'].map(de_tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create vocab","metadata":{}},{"cell_type":"code","source":"from torchtext.vocab import build_vocab_from_iterator\n\nen_vocab = build_vocab_from_iterator(\n    tokenized_en_df,\n    min_freq=2,\n    specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n    special_first=True,\n)\nen_vocab.set_default_index(en_vocab['<unk>'])\n\nde_vocab = build_vocab_from_iterator(\n    tokenized_de_df,\n    min_freq=2,\n    specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n    special_first=True,\n)\nde_vocab.set_default_index(de_vocab['<unk>'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"length of english vocabulary: {len(en_vocab)}\")\nprint(f\"length of german vocabulary: {len(de_vocab)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchtext import transforms\n\nen_transform = transforms.Sequential(\n    ## converts the sentences to indices based on given vocabulary\n    transforms.VocabTransform(vocab=en_vocab),\n\n    ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is 1 as seen in previous section\n    transforms.AddToken(en_vocab['<sos>'], begin=True),\n\n    ## Add <eos> at end of each sentence. 2 because the index for <eos> in vocabulary is 2 as seen in previous section\n    transforms.AddToken(en_vocab['<eos>'], begin=False),\n    \n    ## converts data into tensor\n    transforms.ToTensor(),\n    \n#     ## padding\n#     transforms.PadTransform(\n#         max_length = tokenized_en_df.map(len).max() + 2, #2 for <sos> and <eos>\n#         pad_value = 0)\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchtext import transforms\n\nde_transform = transforms.Sequential(\n    ## converts the sentences to indices based on given vocabulary\n    transforms.VocabTransform(vocab=de_vocab),\n\n    ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is 1 as seen in previous section\n    transforms.AddToken(de_vocab['<sos>'], begin=True),\n    \n    ## converts data into tensor\n    transforms.ToTensor(),\n    \n#     ## padding\n#     transforms.PadTransform(\n#         max_length = tokenized_de_df.map(len).max() + 1, #1 for <sos> \n#         pad_value = 0)\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchtext import transforms\n\nlabel_transform = transforms.Sequential(\n    ## converts the sentences to indices based on given vocabulary\n    transforms.VocabTransform(vocab=de_vocab),\n\n    ## Add <eos> at end of each sentence. 2 because the index for <eos> in vocabulary is 2 as seen in previous section\n    transforms.AddToken(de_vocab['<eos>'], begin=False),\n    \n    ## converts data into tensor\n    transforms.ToTensor(),\n    \n#     ## padding\n#     transforms.PadTransform(\n#         max_length = tokenized_de_df.map(len).max() + 1, #1 for <sos> \n#         pad_value = 0)\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make a dataset","metadata":{}},{"cell_type":"code","source":"# Write a custom dataset class (inherits from torch.utils.data.Dataset)\nfrom torch.utils.data import Dataset\n\n\n# 1. Subclass torch.utils.data.Dataset\nclass En_De_DatasetCustom(Dataset):\n    \n    # 2. Initialize with a target_dir and transform (optional) parameter\n    def __init__(self, df, transform, is_test = False):\n        \n    # 3. Create class attributes\n        # Get all image paths\n        self.df = df\n        # Setup transforms\n        self.transform = transform\n        # Check if df is used for test\n        self.is_test = is_test\n    \n    # 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)\n    def __len__(self) -> int:\n        \"Returns the total number of samples.\"\n        return len(self.df)\n    \n    # 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n    def __getitem__(self, index: int):\n        \"Returns one sample of data, data and label (X, y).\"\n        if self.is_test == False:\n            tokenized_en_text = en_tokenizer(self.df['english'].values[index])\n            tokenized_de_text = de_tokenizer(self.df['german'].values[index])\n            transformed_en_text = self.transform['en'](tokenized_en_text)\n            transformed_de_text = self.transform['de'](tokenized_de_text)\n            transformed_label = self.transform['label'](tokenized_de_text)\n            return transformed_en_text, transformed_de_text, transformed_label # return data, label (X, y)\n        else:\n            tokenized_en_text = en_tokenizer(self.df['english'].values[index])\n            transformed_en_text = self.transform['en'](tokenized_en_text)\n            return transformed_en_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"En_De_Dataset = En_De_DatasetCustom(\n    df = raw_df,\n    transform = {'en': en_transform, \n                 'de': de_transform,\n                 'label': label_transform}\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"En_De_Dataset[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split dataset into train and validate","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader, random_split\n\n# Define the sizes of the splits\ntrain_size = int(0.8 * len(En_De_Dataset))\nval_size = len(En_De_Dataset) - train_size\n\n# Use random_split to split the dataset\ntrain_dataset, val_dataset = random_split(En_De_Dataset, [train_size, val_size])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare dataloader","metadata":{}},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\n\ndef collate_fn(batch):\n    # Separate data and labels\n    en_sequences, de_sequences, labels = zip(*batch)\n    # Pad the sequences\n    padded_en_sequences = pad_sequence(en_sequences, batch_first=True, padding_value=0)\n    padded_de_sequences = pad_sequence(de_sequences, batch_first=True, padding_value=0)\n    padded_labels = pad_sequence(labels, batch_first=True, padding_value=0)\n\n    \n    return padded_en_sequences, padded_de_sequences, padded_labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport os\n\n# Setup the batch size hyperparameter\nBATCH_SIZE = 512\nNUM_CORES = os.cpu_count()\n\n# Turn datasets into iterables (batches)\ntrain_dataloader = DataLoader(\n    train_dataset, # dataset to turn into iterable\n    batch_size=BATCH_SIZE, # how many samples per batch? \n    shuffle=True, # shuffle data every epoch?\n    num_workers = NUM_CORES,\n    pin_memory =True,\n    collate_fn=collate_fn\n    \n)\n\nval_dataloader = DataLoader(\n    val_dataset,\n    batch_size=1,\n    shuffle=False, # don't necessarily have to shuffle the testing data\n    num_workers = NUM_CORES,\n    pin_memory =True,\n    collate_fn=collate_fn\n                            \n)\n\n# Let's check out what we've created\nprint(f\"Dataloaders: {train_dataloader, val_dataloader}\") \nprint(f\"Length of training dataset: {len(train_dataloader.dataset)}\")\nprint(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\nprint(f\"Length of val dataset: {len(val_dataloader.dataset)}\")\nprint(f\"Length of val dataloader: {len(val_dataloader)} batches of {BATCH_SIZE}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_sequences_batch, output_sequences_batch, labels_batch = next(iter(train_dataloader))\ninput_sequences_batch.shape, output_sequences_batch.shape, labels_batch.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_sequences_batch, output_sequences_batch, labels_batch = next(iter(val_dataloader))\ninput_sequences_batch.shape, output_sequences_batch.shape, labels_batch.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class Encoder_Simple_Seq2Seq (torch.nn.Module):\n    def __init__(self, vocab, embedding_size, hidden_state_size):\n        super().__init__()\n        \n        self.embedding = torch.nn.Embedding(\n            num_embeddings = len(vocab), \n            embedding_dim = embedding_size, \n            padding_idx = vocab['<pad>']\n        )\n        \n        self.gru = torch.nn.GRU(\n            input_size = embedding_size,\n            hidden_size = hidden_state_size,\n            batch_first = True,\n            bias = True\n        )\n    \n    def forward(self, encoder_input):\n        # encoder_input_shape = [N,seq_en]\n        \n        out = self.embedding(encoder_input)\n        out = torch.relu(out)\n        # output_embedding_shape = [N,seq_en,embedding_size]\n        \n        out, hn = self.gru(out)\n        # output_gru_shape = [N,seq_en,hidden_size]; hidden_gru_shape = [1,N,hidden_size]\n        \n        return out, hn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BahdanauAttention(torch.nn.Module):\n    def __init__(self, hidden_state_size):\n        super().__init__()\n        \n        self.Va = torch.nn.Linear(hidden_state_size, 1)\n        self.W1 = torch.nn.Linear(hidden_state_size, hidden_state_size)\n        self.W2 = torch.nn.Linear(hidden_state_size, hidden_state_size)\n        \n    def forward(self, encoder_output, decoder_hidden):\n        # encoder_output_shape = [N,seq_en,hidden_size]\n        # decoder_hidden_shape = [1,N,hidden_size]\n        \n        decoder_hidden = decoder_hidden.permute(1,0,2)\n        # decoder_hidden_shape = [N,1,hidden_size]\n        \n        scores = self.Va(torch.tanh(self.W1(decoder_hidden) + self.W2(encoder_output))) # Dáº¥u \"+\" element-wise\n        # self.W1(decoder_hidden)_shape = [N,1,hidden_size]\n        # self.W2(encoder_output)_shape = [N,seq_en,hidden_size]\n        # scores_shape = [N,seq_en,1]\n        \n        a_weights = torch.nn.functional.softmax(scores,dim=1) # Apply softmax over 1-th dimension\n        # a_weights_shape = [N,seq_en,1]\n        \n        a_weights = a_weights.permute(0,2,1) # Just transpose for dot production\n        # a_weights_shape = [N,1,seq_en]\n        \n        context = torch.bmm(a_weights, encoder_output) # bmm is batch matmul [N,a,b] @ [N,b,c] = [N,a,c]\n        # context_shape = [N,1,hidden_size]\n        \n        return context, a_weights","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Att_Decoder (torch.nn.Module):\n    def __init__(self, vocab, embedding_size, hidden_state_size, dropout_p=0.1):\n        super().__init__()\n        \n        self.embedding = torch.nn.Embedding(\n            num_embeddings = len(vocab), \n            embedding_dim = embedding_size, \n            padding_idx = vocab['<pad>']\n        )\n        \n        self.gru = torch.nn.GRU(\n            input_size = embedding_size + hidden_state_size,\n            hidden_size = hidden_state_size,\n            batch_first = True,\n            bias = True\n        )\n        \n        self.attention = BahdanauAttention(hidden_state_size)\n        self.dropout = torch.nn.Dropout(dropout_p)\n                \n    def forward(self, encoder_output, decoder_hidden, decoder_input = None):\n        # encoder_output_shape = [N,seq_en,hidden_size]\n        # decoder_input_shape = [N,seq_de]\n        # decoder_hidden = [1,N,hidden_size]\n        \n        input_embedding = self.embedding(decoder_input)\n        input_embedding = self.dropout(input_embedding)\n        # input_embedding_shape = [N,seq_de,embedding_size]\n        \n        context, a_weights = self.attention(encoder_output, decoder_hidden)\n        # context_shape = [N,1,hidden_size], a_weights_shape = [N,1,seq_en]\n        \n        input_gru = torch.cat((input_embedding, context), dim=2)\n        # CONDITION: seq_de = 1, input_gru_shape = [N,1,embedding_size + hidden_size]\n        \n        out, hn = self.gru(\n            input_gru, # input\n            decoder_hidden # h_0\n        )\n        # output_gru_shape = [N,seq_de,hidden_size]; hidden_gru_shape = [1,N,hidden_size]\n        \n        return out, hn, a_weights","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Attention_Seq2Seq(torch.nn.Module):\n    def __init__(self, \n                 embedding_size, \n                 hidden_state_size, \n                 output_size, \n                 encoder_vocab, decoder_vocab,\n                 encoder = None, decoder = None, \n                 max_length = 5000):\n        \n        super().__init__()\n        \n        self.max_length = max_length\n        self.encoder_vocab = encoder_vocab\n        self.decoder_vocab = decoder_vocab\n        \n        # The core of model\n        if encoder != None:\n            self.encoder = encoder\n        else:\n            self.encoder = Encoder_Simple_Seq2Seq(\n                                vocab = encoder_vocab, \n                                embedding_size = embedding_size, \n                                hidden_state_size = hidden_state_size\n                            )\n            \n        if decoder != None:\n            self.decoder = decoder\n        else:\n            self.decoder = Att_Decoder(\n                                vocab = decoder_vocab, \n                                embedding_size = embedding_size, \n                                hidden_state_size = hidden_state_size\n                            )\n        # The classifier\n        \n        self.classifier = torch.nn.Sequential(\n            torch.nn.LayerNorm(hidden_state_size),\n            torch.nn.Linear(hidden_state_size, hidden_state_size),\n            torch.nn.ReLU(),\n            torch.nn.LayerNorm(hidden_state_size),\n            torch.nn.Linear(hidden_state_size, output_size)\n        )\n        \n        \n        \n    def do_training(self, encoder_input, decoder_input, epoch):\n        # encoder_input_shape = [N,seq_en], decoder_input_shape = [N,seq_de]\n        \n        encoder_output, encoder_hidden = self.encoder(encoder_input)\n        # encoder_output_shape = [N,seq_en,hidden_size]; \n        # encoder_hidden_shape = [1,N,hidden_size]\n        \n        decoder_hidden = encoder_hidden\n        # decoder_hidden_shape = [1,N,hidden_size]\n        \n        decoder_output = []\n        attentions = []\n        \n        # Teacher Forcing\n        for i in range(decoder_input.shape[1]):\n            \n            if i > 0:\n                if epoch >= 200:\n                    if torch.rand(1) < 0.5:\n                        inputs = torch.multinomial(torch.softmax(outputs.squeeze(1), dim=1), \n                                                   num_samples=1, replacement=True)\n                        # input_shapes = [N,1]\n\n            inputs = decoder_input[:,i].unsqueeze(1)\n            # decoder_input[:,i]_shape = [N]; input_shape = [N,1]\n\n            outputs, decoder_hidden, a_weights  = self.decoder(encoder_output, decoder_hidden, inputs)\n            # outputs_shape = [N,1,hidden_size]\n            # decoder_hidden_shape = [1,N,hidden_size]\n            # a_weights_shape = [N,1,seq_en]\n            \n            outputs = self.classifier(outputs)\n            # output_linear_shape = [N,1,vocab_size]\n\n            decoder_output.append(outputs)\n            # decoder_output_shape = (seq_de * [N,1,vocab_size])\n            \n            attentions.append(a_weights)\n            # attentions_shape = (seq_de * [N,1,seq_en])\n                \n        decoder_output = torch.cat(decoder_output, dim = 1)\n        # decoder_output_shape = [N,seq_de,vocab_size]\n    \n        return decoder_output\n    \n    def make_generation(self, encoder_input, search_strategy = 'beam_search', beam_size = 7):\n        \"\"\"\n            Batch_size must be one \n        \"\"\"\n        assert encoder_input.shape[0] == 1, \"batch_size != 1\"\n        \n        if search_strategy == 'beam_search':\n            generated_sequence = self.Beam_Search(beam_size, encoder_input)\n        else:\n            generated_sequence = self.Greedy_Search(encoder_input)\n        \n        return generated_sequence","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Greedy_step(self, output_at_t):\n    # output_at_t = [N,1,vocab_size]\n    \n    top_prob, top_idx = output_at_t.topk(1, dim = 2) \n    # top_idx_shape = [N,1,1]\n\n    top_idx = top_idx.squeeze(1).detach()\n    # top_idx_shape = [N,1]\n\n    return top_idx\n\ndef Greedy_Search(self, encoder_input):\n    # encoder_input_shape = [N,seq_en]\n    \n    encoder_output, encoder_hidden = self.encoder(encoder_input)\n    # encoder_output_shape = [N,seq_en,hidden_size]; encoder_hidden_shape = [1,N,hidden_size]\n        \n    decoder_hidden = encoder_hidden\n    # decoder_hidden_shape = [1,N,hidden_size]\n    \n    \n    batch_size = encoder_input.shape[0]\n    inputs = torch.ones(\n        size = (batch_size, 1),\n        dtype = torch.long,\n        device = device if device == 'cuda' else 'cpu'\n    ) * self.decoder_vocab['<sos>']\n    # input_shape = [N,1]\n\n    \n    generated_sequence = [inputs]\n    \n    for i in range(self.max_length):\n        outputs, decoder_hidden, a_weights  = self.decoder(encoder_output, decoder_hidden, inputs)\n        # outputs_shape = [N,1,hidden_size]\n        # decoder_hidden_shape = [1,N,hidden_size]\n        # a_weights_shape = [N,1,seq_en]\n\n        outputs = self.classifier(outputs)\n        # output_linear_shape = [N,1,vocab_size]\n\n        inputs = self.Greedy_step(torch.softmax(outputs,dim=2))\n        # inputs_shape = [N,1]\n        \n        generated_sequence.append(inputs)\n\n        if inputs.item() == self.decoder_vocab['<eos>']:\n            break\n        \n    generated_sequence = torch.cat(generated_sequence, dim = 1)\n    # generated_sequence_shape = [N,seq_de]\n        \n    return generated_sequence.squeeze(0)\n\nAttention_Seq2Seq.Greedy_step = Greedy_step\nAttention_Seq2Seq.Greedy_Search = Greedy_Search","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Beam_step(self, beam_size, output_at_t):\n    # output_at_t = [N,1,vocab_size]\n    \n    top_prob, top_idx = output_at_t.topk(beam_size, dim = 2) \n    # top_idx_shape = [N,1,beam_size]\n    # top_prob_shape = [N,1,beam_size]\n\n    top_idx = top_idx.squeeze(1).detach()\n    # top_idx_shape = [N,beam_size]\n\n    top_prob = top_prob.squeeze(1).detach()\n    # top_prob_shape = [N,beam_size]\n    \n    return top_idx, top_prob\n\ndef Beam_Search(self, beam_size, encoder_input):\n    # encoder_input_shape = [N,seq_en]\n\n    encoder_output, encoder_hidden = self.encoder(encoder_input)\n    # encoder_output_shape = [N,seq_en,hidden_size]; encoder_hidden_shape = [1,N,hidden_size]\n\n    batch_size = encoder_input.shape[0]\n    inputs = torch.ones(\n        size = (batch_size, 1),\n        dtype = torch.long,\n        device = device if device == 'cuda' else 'cpu'\n    ) * self.decoder_vocab['<sos>']\n    # input_shape = [N,1]\n\n    candidate_sequence = inputs\n    # candidate_sequence_shape = [N,1]\n    \n    score = torch.log(torch.ones(size = [batch_size, 1], \n                                device = device if device == 'cuda' else 'cpu'))\n    # score_shape = [N,1]\n    \n    candidates = [(candidate_sequence, score, encoder_hidden)]\n    \n    finished_candidates = []\n    for i in range(self.max_length):\n        \n        new_candidates = []\n        for candidate_sequence, score, decoder_hidden in candidates:\n            # candidates_shape = beam_size * ()\n            # candidate_sequence_shape = [N,seq]\n            # score_shape = [N,1]\n            # decoder_hidden_shape = [1,N,hidden_size]\n\n            inputs = candidate_sequence[:,-1].unsqueeze(-1)\n            # inputs_shape = [N,1]\n            \n            if inputs.item() == self.decoder_vocab['<eos>']:\n                finished_candidates.append((candidate_sequence, score, decoder_hidden))\n                continue\n            \n            outputs, decoder_hidden, a_weights = self.decoder(encoder_output, decoder_hidden, inputs)\n            # outputs_shape = [N,1,hidden_size]\n            # decoder_hidden_shape = [1,N,hidden_size]\n            # a_weights_shape = [N,1,seq_en]\n            \n            outputs = self.classifier(outputs)\n            # output_linear_shape = [N,1,vocab_size]\n\n            top_tokens, top_probs = self.Beam_step(beam_size = beam_size, \n                                                   output_at_t = torch.softmax(outputs,dim=2))\n            # top_tokens = [N,beam_size]\n            # top_probs = [N,beam_size]\n            \n            for j in range(beam_size):\n                new_candidate_sequence = torch.cat([candidate_sequence, top_tokens[:,j].unsqueeze(-1)], dim = 1) # top_tokens[:,j].unsqueeze(-1) -> shape: [N,1]\n                # new_candidate_sequence_shape = [N,seq+1]\n                new_score = score + torch.log(top_probs[:,j].unsqueeze(-1))  # top_probs[:,j].unsqueeze(-1) -> shape: [N,1]\n                # new_score_shape = [N,1]\n                new_candidates.append((new_candidate_sequence, new_score, decoder_hidden))\n            \n        \n            new_candiadates = sorted(new_candidates, key=lambda x: x[1]/len(x[0]), reverse=True)\n            candidates = new_candidates[:beam_size]\n        \n        \n        if all([candidate_sequence[:,-1].unsqueeze(-1) == self.decoder_vocab['<eos>'] \\\n                for candidate_sequence, _, _ in candidates]):\n            break\n        \n        if len(finished_candidates) == beam_size/2:\n            break\n            \n    candidates.extend(finished_candidates)\n    candidates = sorted(candidates, key=lambda x: x[1]/len(x[0]), reverse=True)\n        \n    return candidates[0][0].squeeze(0)\n\nAttention_Seq2Seq.Beam_step = Beam_step\nAttention_Seq2Seq.Beam_Search = Beam_Search","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_to_sentence(sequence, vocab):\n\n    try:\n        sos_idx = list(sequence).index(vocab['<sos>'])\n    except ValueError as Error:\n        sos_idx = -1\n    \n    try:\n        eos_idx = list(sequence).index(vocab['<eos>'])\n    except ValueError as Error:\n        eos_idx = -1\n        \n    if eos_idx != -1:\n        sequence = vocab.lookup_tokens(list(sequence)[sos_idx+1: eos_idx])\n    else:\n        sequence = vocab.lookup_tokens(list(sequence)[sos_idx+1:])\n        \n    sentences = ' '.join(sequence)\n    \n    return sentences","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1 = Attention_Seq2Seq(\n    embedding_size = 512,\n    hidden_state_size = 512, \n    output_size = len(de_vocab),\n    encoder_vocab = en_vocab,\n    decoder_vocab = de_vocab,\n    max_length = 50\n)\nmodel_1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = input_sequences_batch[0].unsqueeze(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequence = model_1.make_generation(sample.to(device))\nsequence","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence = convert_to_sentence(sequence, de_vocab)\nsentence","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"# Set seeds\ndef set_seeds(seed: int=42):\n    \"\"\"Sets random sets for torch operations.\n\n    Args:\n        seed (int, optional): Random seed to set. Defaults to 42.\n    \"\"\"\n    # Set the seed for general torch operations\n    torch.manual_seed(seed)\n    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n    torch.cuda.manual_seed(seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from timeit import default_timer as timer \n\ndef print_train_time(start: float, end: float, device: torch.device = None):\n    total_time = end - start\n    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n    return total_time","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_step(model: torch.nn.Module,\n               data_loader: torch.utils.data.DataLoader,\n               loss_fn: torch.nn.Module,\n               optimizer: torch.optim.Optimizer,\n               epoch,\n               device: torch.device = 'cpu'):\n    \n    if device != 'cpu':\n        model.to(device)\n    \n    train_loss = 0\n    model.train()\n    for batch, (X_encoder,X_decoder, y) in enumerate(data_loader):\n        # Send data to GPU\n        if device != 'cpu':\n            X_encoder, X_decoder, y = X_encoder.to(device), X_decoder.to(device), y.to(device)\n\n        # 1. Forward pass\n        y_pred = model.do_training(X_encoder, X_decoder, epoch)\n            # y_pred_shape = [N,seq_de,vocab_size]\n        \n        # 2. Calculate loss\n        loss = loss_fn(y_pred.view(-1,y_pred.shape[2]), y.view(-1))\n        train_loss += loss\n\t\t\t\t\n        # 3. Optimizer zero grad\n        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\n        # 5. Optimizer step\n        optimizer.step()\n\n    # Calculate loss and accuracy per epoch and print out what's happening\n    train_loss /= len(data_loader)\n\n    return train_loss\n\ndef test_step(model: torch.nn.Module,\n              data_loader: torch.utils.data.DataLoader,\n              device: torch.device = 'cpu'):\n    \n    if device != 'cpu':\n        model.to(device)\n\n    model.eval() \n    with torch.inference_mode(): \n        \n        X_encoder,X_decoder, y = list(data_loader)[7]\n        \n        # Send data to GPU\n        if device != 'cpu':\n            X_encoder, X_decoder, y = X_encoder.to(device), X_decoder.to(device), y.to(device)\n\n        sequence = model.make_generation(X_encoder)\n\n        # print translation results\n        Generated_sentence = convert_to_sentence(sequence.cpu(), model.decoder_vocab)\n        Input_sentence = convert_to_sentence(X_encoder.squeeze(0).cpu(), model.encoder_vocab)\n        Label_sentence = convert_to_sentence(y.squeeze(0).cpu(), model.decoder_vocab)\n\n            \n    return Input_sentence, Label_sentence, Generated_sentence ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import Dict, List, Tuple\n\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device = 'cpu') -> Dict[str, List]:\n    \n    # Create empty results dictionary\n    results = {\n        \"Training_Loss\": [],\n    }\n\n    # Loop through training and testing steps for a number of epochs\n    for epoch in tqdm(range(epochs)):\n            Training_Loss = train_step(\n                data_loader=train_dataloader, \n                model=model, \n                loss_fn=loss_fn,\n                optimizer=optimizer,\n                epoch=epoch,\n                device=device\n            )\n            Translation_results = test_step(\n                data_loader=test_dataloader,\n                model=model,\n                device=device\n            )\n            # Print out what's happening\n            print(\n                f\"Epoch: {epoch} | Training_Loss: {Training_Loss:.4f} \\n\"\n                f\"Input_sentence: {Translation_results[0]}\\n\"\n                f\"Output_sentence: {Translation_results[1]}\\n\"\n                f\"Generated_sentence: {Translation_results[2]}\\n\"\n            )\n\n            # Update results dictionary\n            results[\"Training_Loss\"].append(Training_Loss.item())\n\n  # Return the filled results at the end of the epochs\n    return results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_results(Training_Loss):\n    plt.figure(figsize=(10,5))\n    \n    plt.plot(Training_Loss, color=\"blue\", label=\"Training Loss\")\n    plt.title(\"Training and Test Loss curves\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss value\")\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setup loss function and optimizer\nloss_fn = nn.CrossEntropyLoss(ignore_index=model_1.decoder_vocab['<pad>']) \noptimizer = torch.optim.SGD(params=model_1.parameters(), lr=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\nset_seeds()\n\n# Measure time\nfrom timeit import default_timer as timer\ntrain_time_start = timer()\n\n# Setup the num_epochs hyperparameter\nNUM_EPOCHS = 700\n\nresults = train(model=model_1,\n                train_dataloader=train_dataloader,\n                test_dataloader=val_dataloader,\n                optimizer=optimizer,\n                loss_fn=loss_fn,\n                epochs=NUM_EPOCHS,\n                device=device)\n\ntrain_time_end = timer()\ntotal_train_time_model_1 = print_train_time(start=train_time_start,\n                                            end=train_time_end,\n                                            device=device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_results(results[\"Training_Loss\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\ntarget_dir_path = Path(\"/kaggle/working/experiment\")\ntarget_dir_path.mkdir(parents=True,exist_ok=True)\nmodel_name = 'simple_seq2seq.pt'\nmodel_save_path = target_dir_path / model_name\ntorch.save(obj=model_1.state_dict(), f=model_save_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make Prediction","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ntest_df = pd.read_csv(\"/kaggle/input/machine-translation-dataset-de-en/translation_test.csv\")\ntest_df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchtext.data.utils import get_tokenizer\n\nen_tokenizer = get_tokenizer(tokenizer = 'spacy', language = \"en_core_web_sm\")\n\n# apply tokenizer to our dataset\ntokenized_en_test_df = test_df['english'].map(en_tokenizer)\ntokenized_en_test_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchtext import transforms\n\ntest_transform = transforms.Sequential(\n    ## converts the sentences to indices based on given vocabulary\n    transforms.VocabTransform(vocab=en_vocab),\n\n    ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is 1 as seen in previous section\n    transforms.AddToken(en_vocab['<sos>'], begin=True),\n\n    ## Add <eos> at end of each sentence. 2 because the index for <eos> in vocabulary is 2 as seen in previous section\n    transforms.AddToken(en_vocab['<sos>'], begin=False),\n    \n    ## converts data into tensor\n    transforms.ToTensor()\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Test_Dataset = En_De_DatasetCustom(\n    df = test_df,\n    transform = {'en': en_transform},\n    is_test = True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport os\n\n# Setup the batch size hyperparameter\nBATCH_SIZE = 1\nNUM_CORES = os.cpu_count()\n\n# Turn datasets into iterables (batches)\ntest_dataloader = DataLoader(\n    Test_Dataset, # dataset to turn into iterable\n    batch_size=BATCH_SIZE, # how many samples per batch? \n    shuffle = False,\n    num_workers = NUM_CORES,\n    pin_memory = True\n    \n)\n# Let's check out what we've created\nprint(f\"Dataloaders: {test_dataloader}\") \nprint(f\"Length of training dataset: {len(test_dataloader.dataset)}\")\nprint(f\"Length of train dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_predictions(model: torch.nn.Module, \n                     data_loader: torch.utils.data.DataLoader, \n                     device: torch.device = 'cpu'):\n    \n    if device != 'cpu':\n        model.to(device)\n   \n    model.eval()\n    with torch.inference_mode():\n        y = []\n        pbar = tqdm(enumerate(data_loader), total=len(data_loader), desc=\"Predicting\")\n        for batch, X_encoder in pbar:\n            # Send data to GPU\n            if device != 'cpu':\n                X_encoder= X_encoder.to(device)\n            \n            sequence = model.make_generation(X_encoder)\n            Generated_sentence = convert_to_sentence(sequence.cpu(), model.decoder_vocab)\n            y.append(Generated_sentence)\n        \n    return y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = make_predictions(\n    model = model_1, \n    data_loader = test_dataloader, \n    device = device)\n    \ntest_df['generated'] = predictions\n\nprint(test_df['english'][7], test_df['german'][7], test_df['generated'][7], sep = '\\n')\nprint()\nprint(test_df['english'][9], test_df['german'][9], test_df['generated'][9], sep = '\\n')\nprint()\nprint(test_df['english'][12], test_df['german'][12], test_df['generated'][12], sep = '\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}